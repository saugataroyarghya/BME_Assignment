{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OKibmtEDWQ5N",
        "outputId": "58f947e3-d66f-4797-f620-f43f62eb09e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7b0a1e3142c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "\n",
            "============================================================\n",
            "PROTEIN SECONDARY STRUCTURE PREDICTION\n",
            "Using REAL CB513 Dataset from Hugging Face\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "PROTEIN 2D STRUCTURE PREDICTION PIPELINE\n",
            "REAL CB513 DATA ONLY - NO SYNTHETIC DATA\n",
            "============================================================\n",
            "\n",
            "1. LOADING REAL CB513 DATASET...\n",
            "\n",
            "============================================================\n",
            "DOWNLOADING CB513 DATASET FROM HUGGING FACE\n",
            "============================================================\n",
            "\n",
            "Downloading CB513.csv from Hugging Face...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-937005260.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;31m# Run the complete pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;31m# Save the model and history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-937005260.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;31m# Step 1: Download and Load CB513 Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n1. LOADING REAL CB513 DATASET...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0mcb513_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_cb513_from_huggingface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcb513_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-937005260.py\u001b[0m in \u001b[0;36mdownload_cb513_from_huggingface\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDownloading CB513.csv from Hugging Face...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ“ Download complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Protein 2D Structure Prediction Using REAL CB513 Dataset from Hugging Face\n",
        "# NO SYNTHETIC DATA - ONLY REAL PROTEINS\n",
        "\n",
        "# ============================================\n",
        "# PART 1: INSTALLATION AND IMPORTS\n",
        "# ============================================\n",
        "\n",
        "!pip install matplotlib seaborn scikit-learn imageio pillow\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import urllib.request\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROTEIN SECONDARY STRUCTURE PREDICTION\")\n",
        "print(\"Using REAL CB513 Dataset from Hugging Face\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# PART 2: DOWNLOAD AND LOAD CB513 FROM HUGGING FACE\n",
        "# ============================================\n",
        "\n",
        "def download_cb513_from_huggingface():\n",
        "    \"\"\"Download CB513 dataset from Hugging Face\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DOWNLOADING CB513 DATASET FROM HUGGING FACE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create data directory\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "\n",
        "    # Download CB513.csv from Hugging Face\n",
        "    url = \"https://huggingface.co/datasets/proteinea/secondary_structure_prediction/resolve/main/CB513.csv\"\n",
        "    filepath = \"data/CB513.csv\"\n",
        "\n",
        "    try:\n",
        "        print(\"\\nDownloading CB513.csv from Hugging Face...\")\n",
        "        urllib.request.urlretrieve(url, filepath)\n",
        "        print(\"âœ“ Download complete!\")\n",
        "\n",
        "        # Load the CSV file\n",
        "        print(\"Loading CSV file...\")\n",
        "        data = pd.read_csv(filepath)\n",
        "        print(f\"âœ“ Dataset loaded: {data.shape[0]} proteins\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Download failed: {str(e)}\")\n",
        "        print(\"\\nPlease manually download from:\")\n",
        "        print(\"https://huggingface.co/datasets/proteinea/secondary_structure_prediction/blob/main/CB513.csv\")\n",
        "        return None\n",
        "\n",
        "def process_cb513_csv(df):\n",
        "    \"\"\"Process CB513 CSV data into sequences and structures\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PROCESSING REAL CB513 DATA\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    sequences = []\n",
        "    structures = []\n",
        "\n",
        "    # Check the structure of the dataframe\n",
        "    print(f\"Columns in dataset: {df.columns.tolist()}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Based on the output, we have:\n",
        "    # 'input' - protein sequences\n",
        "    # 'dssp3' - 3-class secondary structure\n",
        "    # 'dssp8' - 8-class secondary structure\n",
        "\n",
        "    seq_col = 'input'\n",
        "    struct_col = 'dssp3'  # Using 3-class directly\n",
        "\n",
        "    print(f\"\\nUsing columns: '{seq_col}' for sequences, '{struct_col}' for structures\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        seq = str(row[seq_col]).upper().strip()\n",
        "        struct = str(row[struct_col]).upper().strip()\n",
        "\n",
        "        # Clean sequences - keep only valid amino acids\n",
        "        seq = ''.join([aa for aa in seq if aa in 'ACDEFGHIKLMNPQRSTVWY'])\n",
        "\n",
        "        # Ensure structure has only H, E, C\n",
        "        struct = ''.join([ss if ss in 'HEC' else 'C' for ss in struct])\n",
        "\n",
        "        # Only add if lengths match and are reasonable\n",
        "        if len(seq) == len(struct) and 30 < len(seq) < 700:\n",
        "            sequences.append(seq)\n",
        "            structures.append(struct)\n",
        "\n",
        "    print(f\"\\nProcessed {len(sequences)} valid protein sequences\")\n",
        "\n",
        "    if len(sequences) > 0:\n",
        "        # Show statistics\n",
        "        avg_len = np.mean([len(s) for s in sequences])\n",
        "        all_ss = ''.join(structures)\n",
        "        h_pct = 100 * all_ss.count('H') / len(all_ss)\n",
        "        e_pct = 100 * all_ss.count('E') / len(all_ss)\n",
        "        c_pct = 100 * all_ss.count('C') / len(all_ss)\n",
        "\n",
        "        print(f\"\\nDataset Statistics:\")\n",
        "        print(f\"  Total proteins: {len(sequences)}\")\n",
        "        print(f\"  Average sequence length: {avg_len:.1f}\")\n",
        "        print(f\"  Helix (H): {h_pct:.1f}%\")\n",
        "        print(f\"  Sheet (E): {e_pct:.1f}%\")\n",
        "        print(f\"  Coil (C): {c_pct:.1f}%\")\n",
        "\n",
        "        print(f\"\\nSample sequence (first 50 AA): {sequences[0][:50]}\")\n",
        "        print(f\"Sample structure (first 50 SS): {structures[0][:50]}\")\n",
        "\n",
        "    return sequences, structures\n",
        "\n",
        "# ============================================\n",
        "# PART 3: DATA PREPROCESSING\n",
        "# ============================================\n",
        "\n",
        "class ProteinDataProcessor:\n",
        "    def __init__(self, max_len=200):\n",
        "        self.max_len = max_len\n",
        "        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "        self.ss_classes = 'HEC'  # Helix, Sheet (Extended), Coil\n",
        "\n",
        "        # Create mapping dictionaries\n",
        "        self.aa_to_idx = {aa: idx+1 for idx, aa in enumerate(self.amino_acids)}\n",
        "        self.aa_to_idx['PAD'] = 0\n",
        "        self.aa_to_idx['UNK'] = len(self.amino_acids) + 1\n",
        "\n",
        "        self.ss_to_idx = {ss: idx for idx, ss in enumerate(self.ss_classes)}\n",
        "        self.idx_to_ss = {idx: ss for ss, idx in self.ss_to_idx.items()}\n",
        "\n",
        "    def encode_sequence(self, sequence):\n",
        "        \"\"\"Convert amino acid sequence to indices\"\"\"\n",
        "        encoded = []\n",
        "        for aa in sequence[:self.max_len]:\n",
        "            encoded.append(self.aa_to_idx.get(aa, self.aa_to_idx['UNK']))\n",
        "\n",
        "        # Pad sequence\n",
        "        while len(encoded) < self.max_len:\n",
        "            encoded.append(self.aa_to_idx['PAD'])\n",
        "\n",
        "        return np.array(encoded)\n",
        "\n",
        "    def encode_structure(self, structure):\n",
        "        \"\"\"Convert secondary structure to indices\"\"\"\n",
        "        encoded = []\n",
        "        for ss in structure[:self.max_len]:\n",
        "            if ss in self.ss_to_idx:\n",
        "                encoded.append(self.ss_to_idx[ss])\n",
        "            else:\n",
        "                encoded.append(2)  # Default to coil (C)\n",
        "\n",
        "        # Pad structure\n",
        "        while len(encoded) < self.max_len:\n",
        "            encoded.append(2)  # Pad with coil\n",
        "\n",
        "        return np.array(encoded)\n",
        "\n",
        "    def one_hot_encode_sequence(self, encoded_seq):\n",
        "        \"\"\"One-hot encode the sequence\"\"\"\n",
        "        return to_categorical(encoded_seq, num_classes=len(self.aa_to_idx))\n",
        "\n",
        "    def one_hot_encode_structure(self, encoded_struct):\n",
        "        \"\"\"One-hot encode the structure\"\"\"\n",
        "        return to_categorical(encoded_struct, num_classes=len(self.ss_classes))\n",
        "\n",
        "# ============================================\n",
        "# PART 4: MODEL ARCHITECTURE\n",
        "# ============================================\n",
        "\n",
        "def create_protein_model(input_shape, num_classes=3):\n",
        "    \"\"\"Create a hybrid CNN-LSTM model for protein structure prediction\"\"\"\n",
        "    model = models.Sequential([\n",
        "        # Input layer\n",
        "        layers.Input(shape=input_shape),\n",
        "\n",
        "        # CNN layers for local pattern detection\n",
        "        layers.Conv1D(64, 7, padding='same', activation='relu', name='conv1'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        layers.Conv1D(128, 5, padding='same', activation='relu', name='conv2'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        # Bidirectional LSTM for sequence context\n",
        "        layers.Bidirectional(layers.LSTM(64, return_sequences=True, name='lstm1')),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Bidirectional(layers.LSTM(32, return_sequences=True, name='lstm2')),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # Dense layers for classification\n",
        "        layers.TimeDistributed(layers.Dense(32, activation='relu', name='dense1')),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        # Output layer\n",
        "        layers.TimeDistributed(layers.Dense(num_classes, activation='softmax', name='output'))\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================\n",
        "# PART 5: WEIGHT TRACKING FOR GIFS\n",
        "# ============================================\n",
        "\n",
        "class WeightTracker(callbacks.Callback):\n",
        "    def __init__(self, layer_names):\n",
        "        self.layer_names = layer_names\n",
        "        self.weights_history = {name: [] for name in layer_names}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for layer in self.model.layers:\n",
        "            if layer.name in self.layer_names:\n",
        "                weights = layer.get_weights()\n",
        "                if len(weights) > 0:\n",
        "                    self.weights_history[layer.name].append(weights[0].copy())\n",
        "\n",
        "def create_weight_gif(weights_history, layer_name, output_path):\n",
        "    \"\"\"Create GIF showing weight evolution\"\"\"\n",
        "    images = []\n",
        "\n",
        "    for epoch, weights in enumerate(weights_history):\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "        # Reshape weights for visualization\n",
        "        if len(weights.shape) == 3:  # Conv1D weights\n",
        "            weights_2d = weights.reshape(-1, weights.shape[-1])\n",
        "        elif len(weights.shape) == 2:  # Dense weights\n",
        "            weights_2d = weights\n",
        "        else:\n",
        "            weights_2d = weights.reshape(-1, weights.shape[-1] if len(weights.shape) > 1 else 1)\n",
        "\n",
        "        # Limit display size\n",
        "        display_weights = weights_2d[:min(50, weights_2d.shape[0]), :min(50, weights_2d.shape[1])]\n",
        "\n",
        "        im = ax.imshow(display_weights, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
        "        ax.set_title(f'{layer_name} Weights - Epoch {epoch+1}', fontsize=14)\n",
        "        ax.set_xlabel('Output Units')\n",
        "        ax.set_ylabel('Input Units')\n",
        "        plt.colorbar(im, ax=ax)\n",
        "\n",
        "        # Save to temporary file\n",
        "        temp_path = f'temp_{epoch}.png'\n",
        "        plt.savefig(temp_path, dpi=60, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Load and append to images\n",
        "        images.append(Image.open(temp_path))\n",
        "        os.remove(temp_path)\n",
        "\n",
        "    # Save as GIF\n",
        "    if images:\n",
        "        images[0].save(output_path, save_all=True, append_images=images[1:],\n",
        "                      duration=200, loop=0)\n",
        "        print(f\"  âœ“ GIF saved: {output_path}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 6: VISUALIZATION FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def create_all_plots(history, y_true, y_pred):\n",
        "    \"\"\"Create all required visualization plots\"\"\"\n",
        "\n",
        "    # 1. Loss and Accuracy Curves\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2, color='blue')\n",
        "    axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')\n",
        "    axes[0].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].legend(loc='upper right')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy plot\n",
        "    axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='blue')\n",
        "    axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='red')\n",
        "    axes[1].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[1].legend(loc='lower right')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_curves.png', dpi=100, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Helix', 'Sheet', 'Coil'],\n",
        "                yticklabels=['Helix', 'Sheet', 'Coil'],\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Predicted', fontsize=12)\n",
        "    plt.ylabel('Actual', fontsize=12)\n",
        "    plt.savefig('confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def save_model_architecture(model):\n",
        "    \"\"\"Save model architecture diagram\"\"\"\n",
        "    try:\n",
        "        tf.keras.utils.plot_model(\n",
        "            model,\n",
        "            to_file='model_architecture.png',\n",
        "            show_shapes=True,\n",
        "            show_layer_names=True,\n",
        "            rankdir='TB',\n",
        "            expand_nested=True,\n",
        "            dpi=96\n",
        "        )\n",
        "        print(\"âœ“ Model architecture saved as 'model_architecture.png'\")\n",
        "    except:\n",
        "        print(\"âš  Could not generate model architecture diagram\")\n",
        "        print(\"  To fix: !apt-get install graphviz && !pip install pydot\")\n",
        "\n",
        "# ============================================\n",
        "# PART 7: MAIN TRAINING PIPELINE\n",
        "# ============================================\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PROTEIN 2D STRUCTURE PREDICTION PIPELINE\")\n",
        "    print(\"REAL CB513 DATA ONLY - NO SYNTHETIC DATA\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Download and Load CB513 Data\n",
        "    print(\"\\n1. LOADING REAL CB513 DATASET...\")\n",
        "    cb513_df = download_cb513_from_huggingface()\n",
        "\n",
        "    if cb513_df is None:\n",
        "        print(\"\\nError: Could not load CB513 dataset!\")\n",
        "        return None, None, 0\n",
        "\n",
        "    # Process CB513 data\n",
        "    sequences, structures = process_cb513_csv(cb513_df)\n",
        "\n",
        "    if len(sequences) == 0:\n",
        "        print(\"\\nError: No valid sequences found in CB513!\")\n",
        "        return None, None, 0\n",
        "\n",
        "    print(f\"\\nâœ“ Using {len(sequences)} REAL protein sequences from CB513\")\n",
        "\n",
        "    # Step 2: Preprocess Data\n",
        "    print(\"\\n2. PREPROCESSING DATA...\")\n",
        "    processor = ProteinDataProcessor(max_len=300)  # Increased to handle longer sequences\n",
        "\n",
        "    # Encode sequences and structures\n",
        "    print(\"  Encoding sequences...\")\n",
        "    X_encoded = np.array([processor.encode_sequence(seq) for seq in sequences])\n",
        "    y_encoded = np.array([processor.encode_structure(struct) for struct in structures])\n",
        "\n",
        "    # One-hot encode\n",
        "    print(\"  One-hot encoding...\")\n",
        "    X = np.array([processor.one_hot_encode_sequence(x) for x in X_encoded])\n",
        "    y = np.array([processor.one_hot_encode_structure(y) for y in y_encoded])\n",
        "\n",
        "    print(f\"  Input shape: {X.shape}\")\n",
        "    print(f\"  Output shape: {y.shape}\")\n",
        "\n",
        "    # Step 3: Split Data\n",
        "    print(\"\\n3. SPLITTING DATA...\")\n",
        "    # Since CB513 is traditionally a test set, we'll split it for training/validation/test\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"  Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"  Validation set: {X_val.shape[0]} samples\")\n",
        "    print(f\"  Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Step 4: Create Model\n",
        "    print(\"\\n4. CREATING MODEL...\")\n",
        "    model = create_protein_model(\n",
        "        input_shape=(X.shape[1], X.shape[2]),\n",
        "        num_classes=3\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(\"\\nModel Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # Calculate total parameters\n",
        "    total_params = model.count_params()\n",
        "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "\n",
        "    # Step 5: Train Model\n",
        "    print(\"\\n5. TRAINING MODEL ON REAL CB513 DATA...\")\n",
        "    print(\"  This will take several minutes...\")\n",
        "\n",
        "    # Callbacks\n",
        "    tracked_layers = ['conv1', 'lstm1', 'dense1']\n",
        "    weight_tracker = WeightTracker(tracked_layers)\n",
        "\n",
        "    early_stop = callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Model checkpoint\n",
        "    checkpoint = callbacks.ModelCheckpoint(\n",
        "        'best_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=50,  # More epochs since we have less data\n",
        "        batch_size=16,  # Smaller batch size for small dataset\n",
        "        callbacks=[weight_tracker, early_stop, reduce_lr, checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Step 6: Create Weight GIFs\n",
        "    print(\"\\n6. CREATING WEIGHT EVOLUTION GIFS...\")\n",
        "    os.makedirs('gifs', exist_ok=True)\n",
        "\n",
        "    for layer_name in tracked_layers:\n",
        "        if layer_name in weight_tracker.weights_history:\n",
        "            weights = weight_tracker.weights_history[layer_name]\n",
        "            if weights:\n",
        "                create_weight_gif(weights, layer_name, f'gifs/{layer_name}_weights.gif')\n",
        "\n",
        "    # Step 7: Evaluation\n",
        "    print(\"\\n7. EVALUATING MODEL ON TEST SET...\")\n",
        "\n",
        "    # Load best model\n",
        "    if os.path.exists('best_model.keras'):\n",
        "        print(\"  Loading best model...\")\n",
        "        model = keras.models.load_model('best_model.keras')\n",
        "\n",
        "    # Predictions on test set\n",
        "    print(\"  Making predictions...\")\n",
        "    y_pred = model.predict(X_test, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=-1).flatten()\n",
        "    y_true_classes = np.argmax(y_test, axis=-1).flatten()\n",
        "\n",
        "    # Remove padding from evaluation\n",
        "    mask = X_test.sum(axis=-1).flatten() > 0\n",
        "    y_pred_filtered = y_pred_classes[mask]\n",
        "    y_true_filtered = y_true_classes[mask]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
        "    report = classification_report(\n",
        "        y_true_filtered, y_pred_filtered,\n",
        "        target_names=['Helix', 'Sheet', 'Coil'],\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\n  Test Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\n  Classification Report:\")\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    print(report_df.round(3))\n",
        "\n",
        "    # Calculate per-position accuracy\n",
        "    print(\"\\n  Per-Position Statistics:\")\n",
        "    total_positions = len(y_true_filtered)\n",
        "    helix_correct = np.sum((y_true_filtered == 0) & (y_pred_filtered == 0))\n",
        "    sheet_correct = np.sum((y_true_filtered == 1) & (y_pred_filtered == 1))\n",
        "    coil_correct = np.sum((y_true_filtered == 2) & (y_pred_filtered == 2))\n",
        "\n",
        "    print(f\"    Total amino acids evaluated: {total_positions}\")\n",
        "    print(f\"    Helix predictions correct: {helix_correct}\")\n",
        "    print(f\"    Sheet predictions correct: {sheet_correct}\")\n",
        "    print(f\"    Coil predictions correct: {coil_correct}\")\n",
        "\n",
        "    # Step 8: Visualizations\n",
        "    print(\"\\n8. CREATING VISUALIZATIONS...\")\n",
        "    create_all_plots(history, y_true_filtered, y_pred_filtered)\n",
        "\n",
        "    # Step 9: Save Model Architecture\n",
        "    print(\"\\n9. SAVING MODEL ARCHITECTURE...\")\n",
        "    save_model_architecture(model)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETE - REAL CB513 DATA\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nGenerated Files:\")\n",
        "    print(\"  âœ“ gifs/conv1_weights.gif - Conv layer weight evolution\")\n",
        "    print(\"  âœ“ gifs/lstm1_weights.gif - LSTM layer weight evolution\")\n",
        "    print(\"  âœ“ gifs/dense1_weights.gif - Dense layer weight evolution\")\n",
        "    print(\"  âœ“ training_curves.png - Loss and accuracy plots\")\n",
        "    print(\"  âœ“ confusion_matrix.png - Test set confusion matrix\")\n",
        "    print(\"  âœ“ model_architecture.png - Network architecture diagram\")\n",
        "    print(\"  âœ“ protein_2d_model.h5 - Trained model\")\n",
        "    print(\"  âœ“ best_model.keras - Best checkpoint\")\n",
        "    print(\"  âœ“ training_history.json - Training metrics\")\n",
        "\n",
        "    return model, history, accuracy\n",
        "\n",
        "# ============================================\n",
        "# PART 8: RUN THE COMPLETE PIPELINE\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    model, history, test_accuracy = main()\n",
        "\n",
        "    # Save the model and history\n",
        "    if model is not None:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"SAVING FINAL OUTPUTS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Save model in H5 format\n",
        "        model.save('protein_2d_model.h5')\n",
        "        print(\"âœ“ Model saved as 'protein_2d_model.h5'\")\n",
        "\n",
        "        # Save training history\n",
        "        with open('training_history.json', 'w') as f:\n",
        "            json.dump(history.history, f, indent=2)\n",
        "        print(\"âœ“ Training history saved as 'training_history.json'\")\n",
        "\n",
        "        # Create summary statistics\n",
        "        stats = {\n",
        "            'dataset': 'CB513 from Hugging Face',\n",
        "            'total_proteins': len(history.history['loss']) * 16,  # epochs * batch_size approximation\n",
        "            'test_accuracy': float(test_accuracy),\n",
        "            'model_parameters': int(model.count_params()),\n",
        "            'epochs_trained': len(history.history['loss'])\n",
        "        }\n",
        "\n",
        "        with open('training_stats.json', 'w') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "        print(\"âœ“ Training statistics saved as 'training_stats.json'\")\n",
        "\n",
        "        # Create final report\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ASSIGNMENT COMPLETE!\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
        "        print(\"\\nDataset: REAL CB513 proteins from Hugging Face\")\n",
        "    else:\n",
        "        print(\"\\nError: Training failed. Please check the error messages above.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqwBIzTvZ6Lc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}